{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run Hill Climbing models (SELFIES and SMILES)\n",
    "\n",
    "The hill climbing models are contained in a repo. Below are the instructions to pre-train and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "git clone https://github.com/gkwt/hill-climbing-lstm.git\n",
    "cd hill-climbing-lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define fitness function from tartarus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('../..')\n",
    "\n",
    "from tartarus import pce\n",
    "\n",
    "def fitness_function(smi: str):\n",
    "    dipole, hl_gap, lumo, obj, pce_1, pce_2, sas = pce.get_properties(smi)\n",
    "    return pce_1 - sas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "import rdkit.Chem as Chem\n",
    "import selfies as sf\n",
    "import pandas as pd\n",
    "\n",
    "from lstm_climber import SELFIESDataModule, SMILESDataModule, LanguageModel\n",
    "import lstm_climber.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training\n",
    "\n",
    "### Get the training parameters and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "data_path       = '../../datasets/hce.csv'\n",
    "string_type     = 'selfies' # 'smiles'\n",
    "num_workers     = 6\n",
    "\n",
    "# get the data\n",
    "smi_list, sfs_list = utils.get_lists(data_path, sep=',', header=1, smiles_name='smiles')\n",
    "if string_type == 'selfies':\n",
    "    str_list = sfs_list\n",
    "    dm = SELFIESDataModule(str_list, batch_size = 128, num_workers = num_workers)\n",
    "elif string_type == 'smiles':\n",
    "    str_list = smi_list\n",
    "    dm = SMILESDataModule(str_list, batch_size = 128, num_workers = num_workers)\n",
    "else:\n",
    "    raise ValueError('No such string representation.')\n",
    "\n",
    "# print some stuff about the data\n",
    "print(f'You are using the \"{string_type}\" representation.')\n",
    "print(f'Number of molecules: {len(str_list)}')\n",
    "print(f'Length of longest molecule: {dm.len_molecule}')\n",
    "print(f'Length of alphabet: {dm.len_alphabet}')\n",
    "print(f'Alphabet: {dm.alphabet}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = LanguageModel(1024, 3, dm.len_alphabet, dm.len_molecule)\n",
    "\n",
    "# default logger used by trainer\n",
    "logger = CSVLogger(os.path.join(os.getcwd(), 'trained_models'), name=string_type)\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(dirpath = logger.log_dir, filename = 'final_model', monitor = 'val_loss', mode = 'min', verbose=True),\n",
    "    EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=True)\n",
    "]\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator = 'gpu', \n",
    "    devices = torch.cuda.device_count(),\n",
    "    logger=logger,\n",
    "    max_epochs = 100, \n",
    "    callbacks = callbacks,\n",
    "    enable_progress_bar = False\n",
    ")\n",
    "trainer.fit(model, dm)\n",
    "print('Finished training!')\n",
    "\n",
    "metrics = utils.plot_metrics(os.path.join(logger.log_dir, 'metrics.csv'), logger.log_dir)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the climbing algorithm\n",
    "\n",
    "### Climbing parameters, load in pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "model_path      = f'trained_models/{string_type}/version_0/final_model.ckpt'\n",
    "out_path        = f'RESULTS_retrain_{string_type}'\n",
    "num_workers     = 6\n",
    "\n",
    "# control sampling\n",
    "# total number of samples = num_best * samps_per_seed * num_randomize\n",
    "num_generations = 10\n",
    "num_best        = 2             # number of top molecules to generate seeds\n",
    "num_randomize   = 5             # number of randomized smiles\n",
    "samps_per_seed  = 50            # number of samples for each seed\n",
    "num_seed_chars  = None          # number of initial characters to sample from\n",
    "                                # if none, sample characters 1/4 - 3/4 of strings\n",
    "temperature     = 1.1           # < 1.0 is less random, > 1.0 is more random\n",
    "retrain         = True          # retrain the network on new molecules\n",
    "\n",
    "print(f'Total numbers searched per iteration: {num_randomize * num_best * samps_per_seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Climb using defined fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder for results\n",
    "if not os.path.isdir(out_path):\n",
    "    os.mkdir(out_path)\n",
    "\n",
    "### Starting algorithm\n",
    "# Step 1: Gather data, get the top molecules\n",
    "start_df = pd.DataFrame({'smiles': smi_list})        # smiles are canonicalized, no duplicates\n",
    "start_df['fitness'] = start_df['smiles'].apply(fitness_function)\n",
    "topk = start_df.nlargest(num_best, 'fitness')\n",
    "original_best = topk.iloc[0]['fitness']\n",
    "\n",
    "# load model and collector\n",
    "model = LanguageModel.load_from_checkpoint(model_path)\n",
    "collector = pd.DataFrame(columns = ['smiles', 'fitness', 'generation'])\n",
    "best_collector = pd.DataFrame(columns = ['smiles', 'fitness', 'generation'])\n",
    "\n",
    "for gen in range(num_generations):\n",
    "\n",
    "    # load previous model\n",
    "    if gen > 0 and retrain:\n",
    "        model = LanguageModel.load_from_checkpoint(os.path.join(out_path, f'{gen-1}_model.ckpt'))\n",
    "\n",
    "    print(f'Generation {gen}:')\n",
    "    cols = ['smiles', 'selfies'] if string_type == 'selfies' else ['smiles']\n",
    "    new_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    # Step 1: Create seeds\n",
    "    rand_string = []\n",
    "    for i, row in topk.iterrows():      # loop through top molecules\n",
    "        if string_type == 'selfies':\n",
    "            # randomize the smiles before encoding to selfies to increase diversity\n",
    "            # make sure the new selfies can be encoded by the datamodule\n",
    "            rand_sfs = []\n",
    "            while len(rand_sfs) < num_randomize:\n",
    "                smi_list = utils.randomize_smiles(row['smiles'], num_randomize)\n",
    "                sfs_list = [sf.encoder(s) for s in smi_list]\n",
    "                try:\n",
    "                    _ = [dm.encode_string(s) for s in sfs_list]\n",
    "                    rand_sfs.extend(sfs_list)\n",
    "                except:\n",
    "                    print('Error with randomized SELFIES encoding.')\n",
    "            rand_string.extend(rand_sfs)\n",
    "        else:\n",
    "            rand_string.extend([row['smiles']]*num_randomize)\n",
    "\n",
    "    for i, s in enumerate(rand_string):\n",
    "\n",
    "        # Step 2: Select starting seeds from top molecules for sampling\n",
    "        if num_seed_chars is None:\n",
    "            frac = np.random.rand()/2.0 + 0.25      # from 0.25 to 0.75\n",
    "            len_fn = sf.len_selfies if string_type == 'selfies' else len\n",
    "            seed = utils.get_n_char(s, int(len_fn(s) * frac), string_type)\n",
    "        else:\n",
    "            seed = utils.get_n_char(s, num_seed_chars, string_type)\n",
    "        onehot_seed = dm.encode_string(seed)\n",
    "        print(f'Sampling from {seed}: {i+1}/{len(rand_string)}')\n",
    "\n",
    "        # Step 3: Sample from seeds, KEEP duplicates\n",
    "        num_samps = (i+1) * samps_per_seed - len(new_df)\n",
    "        while num_samps > 0:\n",
    "            sampled_molecules = model.sample(onehot_seed, num_samps, temperature = temperature)\n",
    "            if string_type == 'selfies':\n",
    "                new_smiles, new_selfies = dm.logits_to_smiles(sampled_molecules, return_selfies=True)\n",
    "                new = pd.DataFrame({'smiles': new_smiles, 'selfies': new_selfies})\n",
    "            else:\n",
    "                new_smiles = dm.logits_to_smiles(sampled_molecules, canonicalize=False)\n",
    "                new = pd.DataFrame({'smiles': new_smiles})\n",
    "            new_df = pd.concat([new_df, new])\n",
    "            # drop duplicates within a generation, preserve NaNs/invalid smiles (comment to turn off)\n",
    "            # new_df = new_df[ (~new_df.duplicated('smiles')) | (new_df['smiles'].isnull())]      \n",
    "            num_samps = (i+1) * samps_per_seed - len(new_df)\n",
    "            # import pdb; pdb.set_trace()\n",
    "\n",
    "    # Step 4: Calculate new fitnesses and gather results\n",
    "    new_df['fitness'] = new_df['smiles'].apply(fitness_function)\n",
    "    new_df['generation'] = [gen] * len(new_df)\n",
    "    collector = pd.concat([collector, new_df], ignore_index=True)\n",
    "\n",
    "    best_in_gen = new_df.nlargest(1, 'fitness')\n",
    "    best_overall = collector.nlargest(1, 'fitness')\n",
    "    best_overall['generation'] = gen\n",
    "    best_collector = pd.concat([best_collector, best_overall] , ignore_index=True)\n",
    "\n",
    "    print(f'Best in generation:     {best_in_gen[\"smiles\"].iloc[0]}   {best_in_gen[\"fitness\"].iloc[0]}')\n",
    "    print(f'Best overall:           {best_overall[\"smiles\"].iloc[0]}   {best_overall[\"fitness\"].iloc[0]}')\n",
    "\n",
    "    topk = collector.nlargest(num_best, 'fitness')\n",
    "    topk = topk.reset_index(drop = True)\n",
    "\n",
    "    # Step 5: Retrain the network on the new molecules if desired\n",
    "    if retrain:\n",
    "        # create new dataset depending on string type\n",
    "        train_strings = new_df[string_type].dropna().tolist()          # remove invalid molecules before training\n",
    "        if string_type == 'selfies':\n",
    "            dataset = SELFIESDataset(train_strings, dm.vocab, dm.len_molecule)\n",
    "        else:\n",
    "            dataset = SMILESDataset(train_strings, dm.vocab, dm.len_molecule)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size = dm.batch_size)\n",
    "\n",
    "        # set model parameters\n",
    "        model.verbose = True\n",
    "        model.learning_rate = 1e-4          # use a smaller learning rate for fine-tuning\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            trainer = pl.Trainer(\n",
    "                accelerator = 'gpu',\n",
    "                devices = torch.cuda.device_count(),\n",
    "                enable_checkpointing=False,\n",
    "                logger=False, \n",
    "                max_epochs=10, \n",
    "                enable_progress_bar=False,\n",
    "                num_sanity_val_steps = 0\n",
    "            )\n",
    "        else:\n",
    "            trainer = pl.Trainer(\n",
    "                accelerator = 'cpu',\n",
    "                enable_checkpointing=False,\n",
    "                logger=False, \n",
    "                max_epochs=10, \n",
    "                enable_progress_bar=False,\n",
    "                num_sanity_val_steps = 0\n",
    "            )\n",
    "        \n",
    "        # fit the model\n",
    "        trainer.fit(model, train_loader)\n",
    "        trainer.validate(model, train_loader)\n",
    "        trainer.save_checkpoint(os.path.join(out_path, f'{gen}_model.ckpt'))\n",
    "\n",
    "collector.to_csv(os.path.join(out_path, 'all_results.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
